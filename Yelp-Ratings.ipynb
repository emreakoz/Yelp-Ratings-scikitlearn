{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Yelp Star Ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a data set of venue popularities provided by Yelp.  The data set contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. The implementation of the prediction scheme is detailed in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse the incoming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/2/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in json package has a `loads()` function that converts a JSON string into a Python dictionary.  We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` library, but is *substantially* faster (at the cost of non-robust handling of malformed json).  We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       "  'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018',\n",
       "  'hours': {'Tuesday': {'close': '17:00', 'open': '08:00'},\n",
       "   'Friday': {'close': '17:00', 'open': '08:00'},\n",
       "   'Monday': {'close': '17:00', 'open': '08:00'},\n",
       "   'Wednesday': {'close': '17:00', 'open': '08:00'},\n",
       "   'Thursday': {'close': '17:00', 'open': '08:00'}},\n",
       "  'open': True,\n",
       "  'categories': ['Doctors', 'Health & Medical'],\n",
       "  'city': 'Phoenix',\n",
       "  'review_count': 7,\n",
       "  'name': 'Eric Goldberg, MD',\n",
       "  'neighborhoods': [],\n",
       "  'longitude': -111.983758,\n",
       "  'state': 'AZ',\n",
       "  'stars': 3.5,\n",
       "  'latitude': 33.499313,\n",
       "  'attributes': {'By Appointment Only': True},\n",
       "  'type': 'business'},\n",
       " {'business_id': 'JwUE5GmEO-sH1FuwJgKBlQ',\n",
       "  'full_address': '6162 US Highway 51\\nDe Forest, WI 53532',\n",
       "  'hours': {},\n",
       "  'open': True,\n",
       "  'categories': ['Restaurants'],\n",
       "  'city': 'De Forest',\n",
       "  'review_count': 26,\n",
       "  'name': 'Pine Cone Restaurant',\n",
       "  'neighborhoods': [],\n",
       "  'longitude': -89.335844,\n",
       "  'state': 'WI',\n",
       "  'stars': 4.0,\n",
       "  'latitude': 43.238893,\n",
       "  'attributes': {'Take-out': True,\n",
       "   'Good For': {'dessert': False,\n",
       "    'latenight': False,\n",
       "    'lunch': True,\n",
       "    'dinner': False,\n",
       "    'breakfast': False,\n",
       "    'brunch': False},\n",
       "   'Caters': False,\n",
       "   'Noise Level': 'average',\n",
       "   'Takes Reservations': False,\n",
       "   'Delivery': False,\n",
       "   'Ambience': {'romantic': False,\n",
       "    'intimate': False,\n",
       "    'touristy': False,\n",
       "    'hipster': False,\n",
       "    'divey': False,\n",
       "    'classy': False,\n",
       "    'trendy': False,\n",
       "    'upscale': False,\n",
       "    'casual': False},\n",
       "   'Parking': {'garage': False,\n",
       "    'street': False,\n",
       "    'validated': False,\n",
       "    'lot': True,\n",
       "    'valet': False},\n",
       "   'Has TV': True,\n",
       "   'Outdoor Seating': False,\n",
       "   'Attire': 'casual',\n",
       "   'Alcohol': 'none',\n",
       "   'Waiter Service': True,\n",
       "   'Accepts Credit Cards': True,\n",
       "   'Good for Kids': True,\n",
       "   'Good For Groups': True,\n",
       "   'Price Range': 1},\n",
       "  'type': 'business'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ujson as json\n",
    "import gzip\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "data[:2] #venue information is a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit Learn, the labels to be predicted, in this case, the stars, are always kept in a separate data structure than the features.  Let's get in this habit now, by creating a separate list of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5, 4.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_ratings = [row['stars'] for row in data]\n",
    "star_ratings[0:2] # ratings is a list of floats per venue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Number of Venues per City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The venues belong to different cities.  You can imagine that the ratings in some cities are probably higher than others.  Let's build an estimator to make a prediction based on this, but first we need to work out the average rating for each city.  For this problem, we can create a dictionary where the key is the city name and value is the venue counter per city. The collection modeule's `defaultdict` class defaults values for keys taht haven't been used. Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "star_sum = defaultdict(int)\n",
    "count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can increment any key of `stars` or `count` without first worrying whether the key exists.  We need to go through the `data` and `star_ratings` list together, which we can do with the `zip()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, stars in zip(data, star_ratings):\n",
    "    # increment the running sum in star_sum\n",
    "    # increment the running count in count\n",
    "    star_sum[row['city']] += stars\n",
    "    count[row['city']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the average ratings.  Again, a dictionary makes a good container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix 3.6702903946388683\n",
      "De Forest 3.75\n",
      "Mc Farland 3.1\n",
      "Middleton 3.611111111111111\n",
      "Madison 3.6457337883959045\n"
     ]
    }
   ],
   "source": [
    "avg_stars = dict()\n",
    "for city in star_sum:\n",
    "    # calculate average star rating and store in avg_stars\n",
    "    avg_stars[city] = star_sum[city]/count[city]\n",
    "\n",
    "for key in list(avg_stars)[:5]:\n",
    "    print (key, avg_stars[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Ratings of a Venue by the City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a custom estimator that will make a prediction based solely on the city of a venue.  It is tempting to hard-code the answers from the previous section into this model, but we're going to resist and do things properly.\n",
    "\n",
    "This custom estimator will have a `.fit()` method.  It will receive `data` as its argument `X` and `star_ratings` as `y`, and should repeat the calculation of the previous problem there.  Then the `.predict()` method can look up the average rating for the city of each record it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from collections import defaultdict\n",
    "\n",
    "class CityEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = dict()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        star_sum = defaultdict(int)\n",
    "        count = defaultdict(int)\n",
    "\n",
    "        # Store the average rating per city in self.avg_stars\n",
    "        for row, stars in zip(X, y):\n",
    "            star_sum[row['city']] += stars\n",
    "            count[row['city']] += 1\n",
    "        \n",
    "        for city in star_sum:\n",
    "            self.avg_stars[city] = star_sum[city]/count[city]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for row in X:\n",
    "            if row['city'] not in self.avg_stars: # if the city is not in the training set, we return 0 as the rating\n",
    "                result.append(0)\n",
    "            else:\n",
    "                result.append(self.avg_stars[row['city']])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an instance of our estimator, train it and show a few of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6702903946388683, 3.75, 3.75, 3.75, 3.75]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_est = CityEstimator()\n",
    "city_est.fit(data, star_ratings)\n",
    "city_est.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Ratings of a Venue by the Lattitude Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that a city-based model might not be sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  Use the latitude and longitude of a venue as features that help you understand neighborhood dynamics.\n",
    "\n",
    "Instead of writing a custom estimator, we'll use one of the built-in estimators in Scikit Learn.  Since these estimators won't know what to do with a list of dictionaries, we'll build a `ColumnSelectTransformer` that will return an array containing selected keys of our feature matrix.  While it is tempting to hard-code the latitude and longitude in here, this transformer will be more useful in the future if we write it to work on an arbitrary list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and one\n",
    "        # column for each in self.col_names\n",
    "        new_X = []\n",
    "        return [[row[name] for name in self.col_names] for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a single row, just as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33.499313, -111.983758]]\n",
      "[{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA', 'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018', 'hours': {'Tuesday': {'close': '17:00', 'open': '08:00'}, 'Friday': {'close': '17:00', 'open': '08:00'}, 'Monday': {'close': '17:00', 'open': '08:00'}, 'Wednesday': {'close': '17:00', 'open': '08:00'}, 'Thursday': {'close': '17:00', 'open': '08:00'}}, 'open': True, 'categories': ['Doctors', 'Health & Medical'], 'city': 'Phoenix', 'review_count': 7, 'name': 'Eric Goldberg, MD', 'neighborhoods': [], 'longitude': -111.983758, 'state': 'AZ', 'stars': 3.5, 'latitude': 33.499313, 'attributes': {'By Appointment Only': True}, 'type': 'business'}]\n"
     ]
    }
   ],
   "source": [
    "cst = ColumnSelectTransformer(['latitude', 'longitude'])\n",
    "print(cst.fit_transform(data[:1]))\n",
    "print(data[:1])\n",
    "assert (cst.fit_transform(data[:1])\n",
    "        == [[data[0]['latitude'], data[0]['longitude']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed the output of the transformer in to a `sklearn.neighbors.KNeighborsRegressor`.  As a sanity check, we'll test it with the first 5 rows.  To truly judge the performance, we'd need to make a test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 4.2, 4. , 3.8, 4.2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "data_transform = cst.fit_transform(data)\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(data_transform, star_ratings)\n",
    "test_data = data[:5]\n",
    "test_data_transform = cst.transform(test_data)\n",
    "knn.predict(test_data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this by hand, let's make a pipeline that takes a list of (name, transformer-or-estimator) tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('transformer', ColumnSelectTransformer(['latitude','longitude'])), \n",
    "    ('estimator', knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 4.2, 4. , 3.8, 4.2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(data, star_ratings)\n",
    "pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KNeighborsRegressor` takes the `n_neighbors` hyperparameter, which tells it how many nearest neighbors to average together when making a prediction.  There is no reason to believe that 5 is the optimum value.  Determine a better value of this hyperparameter.   There are several ways to do this:\n",
    "\n",
    "1. Use [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) to split your data into a training set and a test set.  Score the performance on the test set.  After finding the best hyperparameter, retrain the model on the full data at that hyperparameter value.\n",
    "\n",
    "2. Use [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to return cross-validation scores on your data for various values of the hyperparameter.  Choose the best one, and retrain the model on the full data.\n",
    "\n",
    "3. Use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to do the splitting, training, and grading automatically.  `GridSearchCV` takes an estimator and acts as an estimator.  You can either give it the `KNeighborsRegressor` directly and put it in a pipeline, or you can pass the whole pipeline into the `GridSearchCV`.  In the latter case, the hyperparameter `param` of an estimator named `est` in a pipeline becomes a hyperparameter of the pipeline with name `est__param`.\n",
    "\n",
    "Let's use GridSearch to perform both cross validation and find the right values for the hyperparameters of our model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('transformer', ColumnSelectTransformer(col_names=['latitude', 'longitude'])), ('estimator', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "          weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'estimator__n_neighbors': [60]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, star_ratings, test_size=0.3)\n",
    "\n",
    "param_grid = {'estimator__n_neighbors': [60] }\n",
    "lat_long_est = GridSearchCV(pipe, param_grid, cv=5)\n",
    "lat_long_est.fit(data, star_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator__n_neighbors': 60}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_long_est.best_params_ #we can look up from the best_params_ dictionary the optimum number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Ratings of a Venue by the Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While location is important, we could also try seeing how predictive the\n",
    "venue's category is.  Lets's build an estimator that considers only the categories.\n",
    "\n",
    "The categories come as a list of strings, but the built-in estimators all need numeric input.  The standard way to deal with categorical features is **one-hot encoding**, also known as dummy variables.  In this approach, each category gets its own column in the feature matrix.  If the row has a given category, that column gets filled with a 1.  Otherwise, it is 0.\n",
    "\n",
    "The `ColumnSelectTransformer` from the previous section can be used to extract the categories column as a list of strings.  scikit learn provides [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which takes in a list of dictionaries.  It creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it.  Missing keys are filled with zeros.  Therefore, we need to only build a transformer that takes a list of strings to a dictionary with keys given by those strings and values of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will come in as a list of lists of lists.  Return a list of\n",
    "        # dictionaries corresponding to those inner lists.\n",
    "        return [{key:1 for key in double_list[0]} for double_list in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should allow this to pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (DictEncoder().fit_transform([[['a']], [['b', 'c']]])\n",
    "        == [{'a': 1}, {'b': 1, 'c': 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a pipeline with your `ColumnSelectTransformer`, your `DictEncoder`, the `DictVectorizer`, and a regularized linear model, like `Ridge`, as the estimator.  This model will have a large number of features, one for each category, so there is a significant danger of overfitting.  Use cross validation to choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cst', ColumnSelectTransformer(col_names=['categories'])), ('denc', DictEncoder()), ('dvec', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)), ('estimator', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'estimator__alpha': [1, 10, 100, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "clf = Ridge()\n",
    "cat_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['categories'])),\n",
    "    ('denc', DictEncoder()),\n",
    "    ('dvec', DictVectorizer()),\n",
    "    ('estimator', clf)\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {'estimator__alpha': [1, 10, 100 ,1000] }\n",
    "category_est = GridSearchCV(cat_pipe, param_grid, cv=5)\n",
    "category_est.fit(data, star_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator__alpha': 10}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_est.best_params_ #we can look up from the best_params_ dictionary the optimum number of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Ratings of a Venue by the Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them with one-hot encoding.  The `DictVectorizer` can do this, but only once we've flattened the dictionary to a single level, like so:\n",
    "```\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "\n",
    "Build a custom transformer that flattens the attributes dictionary.  Place this in a pipeline with a `DictVectorizer` and a regressor.\n",
    "\n",
    "It might be difficult to find a single regressor that does well enough.  A common solution is to use a linear model to fit the linear part of some data, and use a non-linear model to fit the residual that the linear model can't fit.  It can be done as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class dictFlattener(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        '''This transformer flattens the \n",
    "        attributes dictionary.\n",
    "        '''\n",
    "        all_list = []\n",
    "        for lst in X:\n",
    "            flattened = {}\n",
    "            for d in lst:\n",
    "                for k, v in d.items():\n",
    "                    if type(v) == str:\n",
    "                        flattened[k+'_'+v] = 1\n",
    "                    elif type(v) == bool:\n",
    "                        flattened[k] = int(v)\n",
    "                    elif type(v) == int:\n",
    "                        flattened[k+'_'+str(v)] = 1\n",
    "                    else:\n",
    "                        for k_, v_ in v.items():\n",
    "                            flattened[k+'_'+k_] = int(v_)\n",
    "            all_list.append(flattened)\n",
    "    \n",
    "        return all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class doubleLayerModel(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''This class builds a linear model first and uses a non-linear\n",
    "    model to fit the residual that the linear model can't fit.\n",
    "    '''\n",
    "    def __init__(self, base, residual):\n",
    "        self.base = base\n",
    "        self.residual = residual\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.base.fit(X,y)\n",
    "        self.residual.fit(X, y-self.base.predict(X))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.base.predict(X) + self.residual.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cst', ColumnSelectTransformer(col_names=['attributes'])), ('dF', dictFlattener()), ('dvec', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)), ('estimator', doubleLayerModel(base=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   ...mators=150, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "lin_est = Ridge()\n",
    "non_lin_est = RandomForestRegressor(n_estimators=150, max_depth=15, random_state=0)\n",
    "\n",
    "attribute_est = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['attributes'])),\n",
    "    ('dF', dictFlattener()),\n",
    "    ('dvec', DictVectorizer()),\n",
    "    ('estimator', doubleLayerModel(lin_est,non_lin_est))\n",
    "])\n",
    "attribute_est.fit(data, star_ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to an estimator, we will have to turn them into transformers.  (A pipeline can contain at most a single estimator.)  We will build a custom `EstimatorTransformer` class that takes an estimator as an argument.  When `fit()` is called, the estimator should be fit.  When `transform()` is called, the estimator's `predict()` method should be called, and its results returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class EstimatorTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        # What needs to be done here?\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the stored estimator.\n",
    "        # Question: what should be returned?\n",
    "        self.estimator.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        # Use predict on the stored estimator as a \"transformation\".\n",
    "        # Be sure to return a 2-D array.\n",
    "        return np.array(self.estimator.predict(X)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_trans = EstimatorTransformer(city_est)\n",
    "city_trans.fit(data, star_ratings)\n",
    "\n",
    "assert ([r[0] for r in city_trans.transform(data[:5])] == city_est.predict(data[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an instance of the `EstimatorTransformer` for each of the previous four models. Combine these together in a single feature matrix with a\n",
    "[`FeatureUnion`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "lat_long_trans = EstimatorTransformer(lat_long_est)\n",
    "category_est_tans = EstimatorTransformer(category_est)\n",
    "attribute_est_trans = EstimatorTransformer(attribute_est)\n",
    "\n",
    "\n",
    "union = FeatureUnion([('city_trans', city_trans),\n",
    "                      ('lat_long_trans', lat_long_trans),\n",
    "                      ('category_est_tans', category_est_tans), \n",
    "                      ('attribute_est_trans', attribute_est_trans)])\n",
    "        # FeatureUnions use the same syntax as Pipelines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a feature matrix with four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.fit(data, star_ratings)\n",
    "trans_data = union.transform(data[:10])\n",
    "assert trans_data.shape == (10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a pipeline to combine the feature union with a linear regression (or another model) to weight the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureUnion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('city_trans', EstimatorTransformer(estimator=CityEstimator())), ('lat_long_trans', EstimatorTransformer(estimator=GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('trans...'linReg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "full_est = Pipeline([('featureUnion',union),\n",
    "                    ('linReg', lin_reg)])\n",
    "full_est.fit(data, star_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
